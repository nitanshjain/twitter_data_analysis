{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nitanshjain/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import fnmatch\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>twt_created_at</th>\n",
       "      <th>total_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pspatilsbi</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>325</td>\n",
       "      <td>25</td>\n",
       "      <td>2022-11-08 22:08:44+00:00</td>\n",
       "      <td>2704</td>\n",
       "      <td>0</td>\n",
       "      <td>agenda great old god blees end</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'screen_name': 'INCIndia', 'name': 'Congress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ththegde</td>\n",
       "      <td>Kandivali East, Mumbai</td>\n",
       "      <td>582</td>\n",
       "      <td>57</td>\n",
       "      <td>2022-11-08 22:00:49+00:00</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>please allow citizen buy forex investment like...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'screen_name': 'PMOIndia', 'name': 'PMO Indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>rupz_boruah</td>\n",
       "      <td>Chabua, India</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>2022-11-08 21:54:37+00:00</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>please take necessary action neurologist amc d...</td>\n",
       "      <td>[{'text': 'Dr_Dhrubajyoti_Kurmi', 'indices': [...</td>\n",
       "      <td>[{'screen_name': 'MoHFW_INDIA', 'name': 'Minis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>lazizpizza99</td>\n",
       "      <td>Jasola Vihar, New Delhi</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-08 21:28:48+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>sleeping suddenly bed start shaking ignored ke...</td>\n",
       "      <td>[{'text': 'peace', 'indices': [231, 237]}, {'t...</td>\n",
       "      <td>[{'screen_name': 'LtGovDelhi', 'name': 'LG Del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>UtkarshMishra_9</td>\n",
       "      <td>Noida, India</td>\n",
       "      <td>707</td>\n",
       "      <td>1122</td>\n",
       "      <td>2022-11-08 21:14:55+00:00</td>\n",
       "      <td>5764</td>\n",
       "      <td>0</td>\n",
       "      <td>estimated magnitude earthquake affected countr...</td>\n",
       "      <td>[{'text': 'earthquake', 'indices': [137, 148]}...</td>\n",
       "      <td>[{'screen_name': 'ANI', 'name': 'ANI', 'id': 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  tweet_id         username                 location  following  \\\n",
       "0        2         2       pspatilsbi               Bangalore         325   \n",
       "1        3         3         ththegde   Kandivali East, Mumbai        582   \n",
       "2        5         5      rupz_boruah            Chabua, India         14   \n",
       "3        8         8     lazizpizza99  Jasola Vihar, New Delhi         23   \n",
       "4       11        11  UtkarshMishra_9             Noida, India        707   \n",
       "\n",
       "   followers             twt_created_at  total_tweets  retweet_count  \\\n",
       "0         25  2022-11-08 22:08:44+00:00          2704              0   \n",
       "1         57  2022-11-08 22:00:49+00:00          1969              0   \n",
       "2         33  2022-11-08 21:54:37+00:00           309              0   \n",
       "3          1  2022-11-08 21:28:48+00:00             6              0   \n",
       "4       1122  2022-11-08 21:14:55+00:00          5764              0   \n",
       "\n",
       "                                                text  \\\n",
       "0                     agenda great old god blees end   \n",
       "1  please allow citizen buy forex investment like...   \n",
       "2  please take necessary action neurologist amc d...   \n",
       "3  sleeping suddenly bed start shaking ignored ke...   \n",
       "4  estimated magnitude earthquake affected countr...   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [{'text': 'Dr_Dhrubajyoti_Kurmi', 'indices': [...   \n",
       "3  [{'text': 'peace', 'indices': [231, 237]}, {'t...   \n",
       "4  [{'text': 'earthquake', 'indices': [137, 148]}...   \n",
       "\n",
       "                                            mentions  \n",
       "0  [{'screen_name': 'INCIndia', 'name': 'Congress...  \n",
       "1  [{'screen_name': 'PMOIndia', 'name': 'PMO Indi...  \n",
       "2  [{'screen_name': 'MoHFW_INDIA', 'name': 'Minis...  \n",
       "3  [{'screen_name': 'LtGovDelhi', 'name': 'LG Del...  \n",
       "4  [{'screen_name': 'ANI', 'name': 'ANI', 'id': 3...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('/Users/nitanshjain/Documents/Thapar 4th Sem/Machine Learing/Machine_Learning_Project/data/final_dataset.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of duplicates is (658, 12)\n",
      "Shape of dataset after removal of duplicates is (605, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataset before removal of duplicates is {}'.format(tweets_df.shape))\n",
    "tweets_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "print('Shape of dataset after removal of duplicates is {}'.format(tweets_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id            int64\n",
       "tweet_id           int64\n",
       "username          object\n",
       "location          object\n",
       "following          int64\n",
       "followers          int64\n",
       "twt_created_at    object\n",
       "total_tweets       int64\n",
       "retweet_count      int64\n",
       "text              object\n",
       "hashtags          object\n",
       "mentions          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    One function to rule them all, \n",
    "    one function to find them, \n",
    "    One function to bring them all, \n",
    "    and in the darkness bind them; \n",
    "    \"\"\"\n",
    "    print('Shape of dataset before removal of tweets with less than 5 words is {}'.format(df.shape))\n",
    "    \n",
    "    for tweets in df.loc[:,'text']:\n",
    "        # count+=1\n",
    "        # print(tweets)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tweet_id = df.loc[df['text'] == tweets, 'tweet_id'].values[0]\n",
    "        # print(tweet_id)\n",
    "        \n",
    "        # removing links\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        x = [word for word in list_words if not urlparse(word).scheme]\n",
    "        tweets = ' '.join(x)\n",
    "\n",
    "        # contractions handling\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        new_list_words = []\n",
    "        for word in list_words:\n",
    "            new_list_words.append(contractions.fix(word))\n",
    "        list_words = new_list_words\n",
    "        del(new_list_words)\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        # adding space between words and punctuations\n",
    "        tweets = tweets.replace(',', ' ,').replace('.', ' .').replace('?', ' ?').replace('!', ' !')\n",
    "        \n",
    "        # removing hashtags and mentions\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        list_words = [word for word in list_words if word[0] not in ('#', '@')]\n",
    "        list_words = [word for word in list_words if word[0] not in ('â–ª')]\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        # removing punctuations\n",
    "        tweets = tweets.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        #removing emojis\n",
    "        tweets = re.sub(r'[^\\x00-\\x7F]+', ' ', tweets)\n",
    "        \n",
    "        #lower case\n",
    "        tweets = tweets.lower()\n",
    "        \n",
    "        #remove numbers\n",
    "        tweets = re.sub(r'\\d+', '', tweets)\n",
    "        tweets = re.sub(' +', ' ', tweets)\n",
    "        \n",
    "        #removing stopwords\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        filtered_words = [word for word in list_words if word not in stopwords.words('english')]\n",
    "        tweets = ' '.join(filtered_words)\n",
    "        del(filtered_words)\n",
    "        \n",
    "        #lemmatization\n",
    "        lem = WordNetLemmatizer()\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        for word in list_words:\n",
    "            list_words = list(map(lambda x: x.replace(word, lem.lemmatize(word)), list_words))\n",
    "        tweets = ' '.join(list_words)\n",
    "        \n",
    "        #removing individual letters\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        filtered_words = [word for word in list_words if len(word)>2]\n",
    "        tweets = ' '.join(filtered_words)\n",
    "        del(filtered_words)\n",
    "        \n",
    "        # updating tweets in dataframe\n",
    "        df.loc[df['tweet_id']==tweet_id, 'text'] = tweets\n",
    "        \n",
    "        #remove small tweets\n",
    "        list_words = tokenizer.tokenize(tweets)\n",
    "        if len(list_words) <= 5:\n",
    "            ind_num = df[df['tweet_id']==tweet_id].index\n",
    "            df.drop(ind_num, inplace=True)\n",
    "        # break\n",
    "    print('Shape of dataset after removal of tweets with less than 5 words is {}'.format(df.shape))\n",
    "    \n",
    "    return df\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removal of tweets with less than 5 words is (605, 12)\n",
      "Shape of dataset after removal of tweets with less than 5 words is (605, 12)\n"
     ]
    }
   ],
   "source": [
    "tweets_df = data_preprocessing(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets_df into a csv file\n",
    "filename = '/Users/nitanshjain/Documents/Thapar 4th Sem/Machine Learing/Machine_Learning_Project/data/tweets_preprocessed_1.csv'\n",
    "tweets_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
